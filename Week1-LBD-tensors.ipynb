{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "In deep learning, you will represent your data using tensors. PyTorch (by Meta) and TensorFlow (by Google) are two of the most commonly used libraries for tensor operations and deep learning. In this course, like most courses in academia, we choose PyTorch because of its benefits in dynamic and flexible programming. Now, let's learn what tensors are and what we can do with them.\n",
    "\n",
    "You should be familiar with vectors and matrices, which are 1D and 2D structures for data. A tensor is simply a generalization of these concepts to any number of dimensions (or axes).Each element of a vector is located by one index, and each element in a matrix is located using 2 indexes. For a tensor, we need an index for each dimension (axis) to locate a certain element. A vector or a matrix are actually special cases of tensors with 1 or 2 dimensions respectively.\n",
    "\n",
    "If you are familiar with numpy, the N-dimensional array (`numpy.ndarray`) is basically the same concept as a tensor. However, tensor libraries like PyTorch are equipped with automatic differentiation, which is central to deep learning. \n",
    "\n",
    "You will need to create and modify tensors, apply element-wise operations, and most importantly, apply tensor multiplications. We will go over the necessary concepts you need to get comfortable working with tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a tensor manually\n",
    "The most common way to define a tensor manually, or represent an already available data as tensors is to use `torch.tensor()` or `torch.as_tensor()`. `torch.tensor()` always creates a new tensor from the provided data, while `torch.as_tensor()` is more memory efficient. If you do not change the properties of the data (dtype and device, which will be discussed more later on), this will not create a copy of the data as tensor, but rather point to the same data in the hardware as a tensor. Therefore, if you modify its elements, you will modify the original data as well. You can see an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# A 0-dimensional float tensor\n",
    "T0 = torch.tensor(\n",
    "    1.0,\n",
    ")\n",
    "\n",
    "# A 1-dimensional boolean tensor\n",
    "T1 = torch.tensor(\n",
    "    [True, False]\n",
    ")\n",
    "\n",
    "# A 2-dimensional integer tensor\n",
    "T2 = torch.tensor([\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6],\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.as_tensor()\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "# A 3-dimensional numpy array. Let's convert it to a tensor \n",
    "A3 = np.array([\n",
    "    [\n",
    "        [1., 2., 3.,],\n",
    "        [4., 5., 6.,]\n",
    "    ],\n",
    "    [\n",
    "        [7., 8., 9.,],\n",
    "        [10., 11., 12.,]\n",
    "    ],\n",
    "    [\n",
    "        [13., 14., 15.,],\n",
    "        [16., 17., 18.,]\n",
    "    ]\n",
    "])\n",
    "\n",
    "T3 = torch.as_tensor(\n",
    "    A3\n",
    ")\n",
    "\n",
    "# Let's change T3\n",
    "T3[0,0,0] = 1000.\n",
    "# we change T3, but A3 also changes. Because they point to the same data in memory\n",
    "print(A3[0,0,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential attributes of tensors\n",
    "You always have to pay close attention to the essential attributes of your tensors, which are:\n",
    "- **shape**: (also referred to as size) the number of indexes in each axis in order, similar to the size of a matrix. Your shape will contain `ndim` numbers, each corresponding to one dimension.\n",
    "- **device**: the hardware where the data represented by the tensor is stored. CPU is `cpu` and GPU is `cuda` or `cuda:0` or `cuda:i` where `i` is the index of the GPU if you are using mroe than one.\n",
    "- **dtype**: The data type of the tensor (bool, int32, int64, float32, float64, ...)\n",
    "- **require_grad**: whether PyTorch needs to keep track of the gradient (more accurately, the gradient of some thing with respect to this tensor)\n",
    "\n",
    "Below is a helpful function to inspect the information about your tensors. You will find this very helpful when debugging code. When performing calculations with tensors, they need to be on the same device, and their dtypes and shapes should be compatible with each other! Sometimes PyTorch takes care of the data types for you, but you are always responsible to keep tensors on the intended device. It is also a good practice to explicitly keep track of the dtypes and not fully trust PyTorch with it, as this may lead to unexpected behavior. You will gain more experience with these as you progress through the course. For now, let's inspect some manually defined tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def print_tensor_info(\n",
    "        name: str, \n",
    "        T: torch.Tensor\n",
    "        ):\n",
    "    ndim = T.ndim # or T.dim()\n",
    "    shape = T.shape # or T.size()\n",
    "    device = T.device\n",
    "    dtype = T.dtype\n",
    "    grad = T.requires_grad\n",
    "    print(f\"Tensor: {name}\")\n",
    "    print(20*\"-\")\n",
    "    print(f\"ndim: {ndim}\")\n",
    "    print(f\"shape: {shape}\")\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"dtype: {dtype}\")\n",
    "    print(f\"requires_grad: {grad}\")\n",
    "    print(20*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 0-dimensional tensor\n",
    "T0 = torch.tensor(\n",
    "    1.0,\n",
    ")\n",
    "print_tensor_info('T0', T0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = torch.tensor(\n",
    "    [True, False]\n",
    ")\n",
    "print_tensor_info('T1', T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = torch.tensor([\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6],\n",
    "        ])\n",
    "print_tensor_info('T2', T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The default float dtype in numpy is float64 which takes up double the memory!\n",
    "If you create a tensor from a numpy array, the dtype will be the same as the numpy array.\n",
    "\n",
    "Since PyTorch uses float32 as default, make sure to convert your data to float32\n",
    "to save memory and computation time.\n",
    "\"\"\"\n",
    "# A 3-dimensional numpy array. Let's convert it to a tensor \n",
    "A3 = np.array([\n",
    "    [\n",
    "        [1., 2., 3.,],\n",
    "        [4., 5., 6.,]\n",
    "    ],\n",
    "    [\n",
    "        [7., 8., 9.,],\n",
    "        [10., 11., 12.,]\n",
    "    ],\n",
    "    [\n",
    "        [13., 14., 15.,],\n",
    "        [16., 17., 18.,]\n",
    "    ]\n",
    "])\n",
    "\n",
    "T3 = torch.as_tensor(A3)\n",
    "print_tensor_info('T3', T3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the dtype and device of tensors\n",
    "\n",
    "You will need to modify tensors and their proeprties. The most useful method for this is `.to()`. You can specify a dtype and/or device, or another tensor with the desired dtype and device. You cannot change the tensor to any arbitrary shape, becuase the total number of elements of the tensor needs to stay the same. Modifying the shapes are more tricky, and will be discussed later. \n",
    "\n",
    "**Important**: By default, neural network parameters are defined with `dtype=torch.float32` so it is best to make sure all your data and other tensors are turned to this dtype before being used by your defined models.\n",
    "\n",
    "**Important**: Virtually all operations between tensors need them to be on the same device. Tee default device is `cpu`, so if you are not utilizing a GPU, you will probably be fine. However, remember to move everything to the same device (in most cases the GPU) before executing any calculations with them. The GPU device is referred to as `cuda` in PyTorch.\n",
    "\n",
    "Probably, the most practicaly way is to use another tensor, usually one that you are going to do some calculations with. Here are some examples of changing the dtypes and devices of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# we will see how we can change the attributes of T1\n",
    "T1 = torch.tensor([1, 2, 3])\n",
    "\n",
    "T2 = torch.tensor([4., 5.], device='cuda', requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING DTYPE TO FLOAT\n",
    "print_tensor_info('T1', T1)\n",
    "print_tensor_info('T1.float()', T1.float())\n",
    "print_tensor_info('T1.type(torch.float)', T1.type(torch.float))\n",
    "print_tensor_info('T1.to(torch.float)', T1.to(torch.float))\n",
    "\n",
    "print(20*'=',\"\\nBEST WAY:\")\n",
    "print_tensor_info('T1.to(torch.float32)', T1.to(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING TO CUDA (GPU)\n",
    "\n",
    "print_tensor_info('T1.cuda()', T2.cuda())\n",
    "\n",
    "print(20*'=',\"\\nBEST WAY:\")\n",
    "print_tensor_info('T1.to(\"cuda\")', T2.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGING TO WHATEVER T2 is (THE EASIEST WAY)\n",
    "print_tensor_info('T1.to(T2)', T1.to(T2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating template tensors\n",
    "\n",
    "There are some predefined tensors like tensors full of zeros, ones, a certain value, or sampled from a certain distribution. You can always find these with a quick Google search or asking ChatGPT :) but you will naturally remember them after using them frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Zeros or ones with a desired shape(size)\n",
    "# providing the shape is mandatory\n",
    "# The default dtype is float32, and default device is cpu\n",
    "ones = torch.ones(size=(3,4), dtype=torch.float32, device='cpu')\n",
    "zeros = torch.zeros(size=(3,4), dtype=torch.float32, device='cpu')\n",
    "\n",
    "\"\"\"\n",
    "These ones are pretty useful\n",
    "\"\"\"\n",
    "# creating tensors with the same dtype, device, shape as another tensor\n",
    "# You can still change dtype, device\n",
    "T = torch.tensor([1, 2, 3, 4])\n",
    "ones_like = torch.ones_like(T) \n",
    "zeros_like = torch.zeros_like(T)\n",
    "\n",
    "# full tensors\n",
    "full = torch.full(size=(3,4), fill_value=3.14, dtype=torch.float32, device='cpu')\n",
    "full_like = torch.full_like(T, fill_value=3.14)\n",
    "\n",
    "# arange\n",
    "# start, end, step\n",
    "# default dtype is int64\n",
    "arange = torch.arange(0, 10, 2) # the end is exclusive\n",
    "# [0 2 4 6 8]\n",
    "\n",
    "# linspace\n",
    "# start, end, number of points\n",
    "\n",
    "linspace = torch.linspace(0, 10, 5)\n",
    "# [0., 2.5, 5., 7.5, 10.]\n",
    "\n",
    "# Try printing their info and see for yourself!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Random Tensors \"\"\"\n",
    "\n",
    "# random numbers from a uniform distribution\n",
    "# between 0 and 1\n",
    "rand = torch.rand(size=(3,4), dtype=torch.float32, device='cpu')\n",
    "\n",
    "# random numbers from a normal distribution\n",
    "# with mean 0 and variance 1\n",
    "randn = torch.randn(size=(3,4), dtype=torch.float32, device='cpu')\n",
    "# you can change mean and std by a simple multiplication and addition\n",
    "\n",
    "# random integers\n",
    "randint = torch.randint(low=0, high=10, size=(3,4), dtype=torch.int64, device='cpu')\n",
    "\n",
    "# random permutation\n",
    "# random permutation of integers from 0 to 9\n",
    "randperm = torch.randperm(n=10, dtype=torch.int64, device='cpu')\n",
    "\n",
    "# Try printing their info and content and see for yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing and slicing tensors\n",
    "\n",
    "Slicing and indexing tensors are very simple and similar to numpy if you are already familiar with it. You can use the `:` notationin ways like this to refer to a certain slice the tensor along some dimension.\n",
    "- `:` means all elements\n",
    "- `start:end` specified the inclusive starting index and ending exclusive index of the slice. If any of them is not defined, it is assumed to be the very beginning (for start) or the very end (for end)!\n",
    "- `start:end:step` begins from starts and selects the elements with the specified step size. If not specified, the default step size is 1.\n",
    "- You can also use the `slice` function if you are looking to define your slices deparately, and then index your tensors with them.\n",
    "\n",
    "remember that negative indexes start from the end. For example, -1 refers to the last location, -2 second last, and so on. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "T = torch.randn(5, 6)\n",
    "\n",
    "print('T:')\n",
    "print(T)\n",
    "print(20*'-')\n",
    "\n",
    "# some examples of slicing:\n",
    "\n",
    "# get the first row\n",
    "print('First row:')\n",
    "print(T[0])\n",
    "print(20*'-')\n",
    "\n",
    "# get the first column\n",
    "print('First column:')\n",
    "print(T[:,0])\n",
    "print(20*'-')\n",
    "\n",
    "# get the first two rows\n",
    "print('First two rows:')\n",
    "print(T[:2])\n",
    "print(20*'-')\n",
    "\n",
    "# Get even columns of all rows:\n",
    "print('Even columns:')\n",
    "print(T[:,::2])\n",
    "print(20*'-')\n",
    "\n",
    "# Get last two columns of the first two rows\n",
    "print('Last two columns of the first two rows:')\n",
    "print(T[:2,-2:])\n",
    "print(20*'-')\n",
    "\n",
    "# example with slice:\n",
    "\n",
    "our_slice =  slice(1, None, 2) # equivalent to 1::2\n",
    "print('the odd rows:')\n",
    "print(T[our_slice])\n",
    "print(20*'-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Indexing with other tensors!\n",
    "\"\"\"\n",
    "\n",
    "T1 = torch.randn(3, 4)\n",
    "print('T1:')\n",
    "print(T1)\n",
    "print(20*'-')\n",
    "\n",
    "# In order to index T1 with another tensor, the two tensors must have the same shape in the shared dimensions\n",
    "# Then, the content of the index tensor will be used to index the tensor to be indexed.\n",
    "\n",
    "# Let's create a tensor with the same shape as T1\n",
    "# and fill it with random integers from 0 to 6\n",
    "index_tensor = torch.randint(low=0, high=3, size=(3,))\n",
    "print('index_tensor:')\n",
    "print(index_tensor)\n",
    "print(20*'-')\n",
    "\n",
    "T1_indexed = T1[index_tensor]\n",
    "print('T1 indexed:')\n",
    "print(T1_indexed)\n",
    "print(20*'-')\n",
    "\n",
    "# you can also do this with random dimensions in the middle of the tensor.\n",
    "\n",
    "T2 = torch.randn(3, 4, 5, 6)\n",
    "\n",
    "index_tensor_dim1 = torch.randint(low=0, high=4, size=(7, 8,))\n",
    "index_tensor_dim2 = torch.randint(low=0, high=5, size=(7, 8,))\n",
    "\n",
    "T2_indexed = T2[:, index_tensor_dim1, index_tensor_dim2, :]\n",
    "\n",
    "print('T2 shape:')\n",
    "print(T2.shape)\n",
    "print(20*'-')\n",
    "\n",
    "print('T2_indexed shape:')\n",
    "print(T2_indexed.shape)\n",
    "print(20*'-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Indexing with a boolean tensor (masking)\n",
    "\"\"\"\n",
    "\n",
    "T = torch.randn(3, 4)\n",
    "print('T:')\n",
    "print(T)\n",
    "print(20*'-')\n",
    "\n",
    "# Let's create a boolean tensor with the same shape as T\n",
    "\n",
    "mask = T <= 0\n",
    "print('mask:')\n",
    "print(mask)\n",
    "print(20*'-')\n",
    "\n",
    "# Now we can index T with the mask\n",
    "T_masked = T[mask]\n",
    "\n",
    "print('T masked:')\n",
    "print(T_masked)\n",
    "print(20*'-')\n",
    "\n",
    "\"\"\"\n",
    "The masked dimensions will be flattened!\n",
    "However, you can use this index to assign new values to the masked elements. Like this:\n",
    "\"\"\"\n",
    "\n",
    "T[mask] = 0\n",
    "print('T after masking:')\n",
    "print(T)\n",
    "print(20*'-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating the shape of tensors\n",
    "\n",
    "This is probably the most tricky job with tensors. Here we go over the most common and frequent ways that tensors have their shapes modified or rearranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Combining and splitting dimensions of a tensor using flatten and reshape:\n",
    "\"\"\"\n",
    "\n",
    "# flatten is used to flatten several dimensions into one by rolling the higher dimensions into the lower ones\n",
    "T = torch.randn(5, 6, 7, 8, 9, 10)\n",
    "\n",
    "T_f1 = T.flatten()\n",
    "print('T.flatten() has shape', T_f1.shape)\n",
    "\n",
    "# flatten with start_dim and end_dim (both inclusive)\n",
    "T_f2 = T.flatten(start_dim=1, end_dim=2)\n",
    "print('T.flatten(start_dim=1, end_dim=2) has shape', T_f2.shape)\n",
    "\n",
    "# reshape to turn it back\n",
    "T_r3 = T_f2.reshape(5, 6, 7, 8, 9, 10)\n",
    "print('T_f2.reshape(5, 6, 7, 8, 9, 10) has shape', T_r3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changing the order of the dimensions\n",
    "\"\"\"\n",
    "\n",
    "T1 = torch.randn(5, 6, 7, 8, 9, 10)\n",
    "\n",
    "# torch.movedim and torch.moveaxis (the same)\n",
    "T2 = T1.movedim(0, 2)\n",
    "print('T1.movedim(0, 2) has shape', T2.shape)\n",
    "\n",
    "T3 = T1.moveaxis([1, 2], [-2, -1])\n",
    "print('T2.moveaxis([1, 2], [-2, -1]) has shape', T3.shape)\n",
    "\n",
    "# torch.transpose to swap two dimensions\n",
    "T4 = T1.transpose(1, 2)\n",
    "print('T1.transpose(1, 2) has shape', T4.shape)\n",
    "\n",
    "# torch.permute to permute the dimensions\n",
    "T5 = T1.permute(1, 0, 3, 2, 5, 4)\n",
    "print('T1.permute(1, 0, 3, 2, 5, 4) has shape', T5.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding and removing extra dimensions\n",
    "\"\"\"\n",
    "\n",
    "T = torch.randn(5, 6, 7)\n",
    "\n",
    "T1 = T.unsqueeze(1)\n",
    "print('T.unsqueeze(1) has shape', T1.shape)\n",
    "\n",
    "# My favorite way is indexing with None\n",
    "T2 = T[:, None, :, :]\n",
    "print('T[:, None, :, :] has shape', T2.shape)\n",
    "\n",
    "T3 = T[None, :, None, :, None, :]\n",
    "print('T[None, :, None, :, None, :] has shape', T3.shape)\n",
    "\n",
    "# Removing extra dimensions:\n",
    "T3_squeezed = T3.squeeze()\n",
    "print('T3.squeeze() has shape', T3_squeezed.shape)\n",
    "\n",
    "# removing the extra dimensions at specific positions\n",
    "T3_squeezed_0_2 = T3.squeeze(0)\n",
    "print('T3.squeeze(0).squeeze(2) has shape', T3_squeezed_0_2.shape)\n",
    "\n",
    "# or you can simply index it with 0\n",
    "T3_squeezed_indexed = T3[0, :, 0, :, 0, :]\n",
    "print('T3[0, :, 0, :, 0, :] has shape', T3_squeezed_indexed.shape)\n",
    "\n",
    "# you can use ... if there are too many dimensions\n",
    "\n",
    "# adding a dimension at the beginning and the end\n",
    "T4 = T3[None, ..., None]\n",
    "print('T3[None, ..., None] has shape', T4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attaching tensors together\n",
    "\n",
    "You can stack or concatenate a list of tensors using `torch.stack()` and `torch.cat()` along a specific dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Concatenate along a specific dimension (other dimensions must match)\n",
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(5, 6, 8)\n",
    "T12_cat = torch.cat([T1, T2], dim=2)\n",
    "print('torch.cat([T1, T2], dim=2) has shape', T12_cat.shape)\n",
    "\n",
    "\n",
    "# Stacking several tensors of the same shape along a new dimension\n",
    "# dimensions must match\n",
    "T3 = torch.randn(4, 5, 6)\n",
    "T4 = torch.randn(4, 5, 6)\n",
    "T5 = torch.randn(4, 5, 6)\n",
    "T345_stack = torch.stack([T3, T4, T5], dim=0)\n",
    "print('torch.stack([T3, T4, T5], dim=0) has shape', T345_stack.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor calculations\n",
    "\n",
    "You will perform calculations with Tensors, and you have to make sure shapes, dtypes and devices are sorted out so the calculations are executed as intended. Most operations are scalar and arithmetic operations. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(5, 6, 7)\n",
    "\n",
    "# element-wise operations:\n",
    "T_sum = T1 + T2\n",
    "T_diff = T1 - T2\n",
    "T_prod = T1 * T2\n",
    "T_div = T1 / T2\n",
    "T_pow = T1 ** T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the shapes don't match?\n",
    "In many occasions, some tensor may not have the same exact shape as the other tensor in the calculation. For the remaining dimensions, we want to generalize the same calculation for all indexes in those dimnension. That is done using a mechanism called **Broadcasting**. Here are some examples of how you can modify the tensors to make them broadcastable! \n",
    "\n",
    "Here is a simple explanation of how broadcasting works:\n",
    "- First, the two tensors' shapes are put on top of each other, starting from the last dimension\n",
    "- If the two dimensions are the same, there is no problem\n",
    "- If we have run out of dimensions on one tensor or the dimension is of size 1, the tensor is broadcased (replicated) with enough copies so that it becomes of the same size as the other tensor.\n",
    "- This continues until all dimensions are figured out.\n",
    "\n",
    "You may get this better with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(7)\n",
    "\n",
    "# broadcasting\n",
    "# T1: [5, 6, 7]\n",
    "# T2: [      7]\n",
    "# go over the above mechanism to see how it works\n",
    "# This is broadcastable so you will not get an error\n",
    "T1_plus_T2 = T1 + T2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(6)\n",
    "\n",
    "# we want to multiply across the dimension with size 6\n",
    "\n",
    "# T1: [5, 6, 7]\n",
    "# T2: [      6]\n",
    "\n",
    "# Broadcasting starts from the last dimension\n",
    "# There is a mismatch in the last dimension\n",
    "# So, it will not work\n",
    "\n",
    "T1_times_T2 = T1 * T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we add a dummy dimension to T2?\n",
    "\n",
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(6)\n",
    "\n",
    "T2 = T2[..., None] # adding a dummy dimension at the end\n",
    "\n",
    "# T1: [5, 6, 7]\n",
    "# T2: [   6, 1]\n",
    "\n",
    "# Now it is broadcastable\n",
    "\n",
    "T1_times_T2 = T1 * T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(5)\n",
    "\n",
    "# T1: [5, 6, 7]\n",
    "# T2: [      5]\n",
    "\n",
    "# There is a mismatch in the second dimension\n",
    "# So, it will not work\n",
    "\n",
    "T1_times_T2 = T1 * T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we add dummy dimensions to T2?\n",
    "\n",
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(5)\n",
    "\n",
    "T2 = T2[:, None, None] # adding two dummy dimensions at the end\n",
    "\n",
    "# T1: [5, 6, 7]\n",
    "# T2: [5, 1, 1]\n",
    "\n",
    "# Now it is broadcastable\n",
    "\n",
    "T1_times_T2 = T1 * T2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way is to move the dimension of T1 to the end and then put it back:\n",
    "\n",
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(5)\n",
    "\n",
    "T1 = T1.movedim(0, 2)\n",
    "\n",
    "# T1: [6, 7, 5]\n",
    "# T2: [      5]\n",
    "\n",
    "# Now it is broadcastable\n",
    "\n",
    "T1_times_T2 = T1 * T2\n",
    "\n",
    "# Put it back to the original shape\n",
    "T1 = T1.movedim(2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to summarize, broadcasting starts from the last dimension. For broadcasting to be successful, you have to face one of these situations for every dimension:\n",
    "- Both tensors are of the same size at this dimension, calculations will proceed normally\n",
    "- One of the tensors has size of 1 in this dimension, so it will be broadcasted\n",
    "- You run out of dimensions on one of the tensors, so the remaining dimensions will be broadcasted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor and Matrix Multiplication\n",
    "Finally at the heart of neural networks, we have tensor multipliation, which is simply a generalization of matrix multiplication. Since tensors can have more than 2 dimensions, tensor multiplications may seem more complicated, but it is not as difficult as it may seem. Just like matrix multiplication, A certain dimension from the two tensors are used for a inner product operation across all the remaining dimensions of both tensors. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "Matrix multiplication\n",
    "\"\"\"\n",
    "\n",
    "T1 = torch.randn(5, 6)\n",
    "T2 = torch.randn(6, 7)\n",
    "\n",
    "# matrix multiplication with 2D tensors (matrices) using @ or torch.matmul\n",
    "T_matmul = T1 @ T2\n",
    "\n",
    "print('T1 @ T2 has shape', T_matmul.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "grouped or batched matrix multiplication:\n",
    "\"\"\"\n",
    "\n",
    "# torch.matmul and @ consider the last two dimensions as the matrices\n",
    "# The remaining dimensions should be broadcastable!\n",
    "T1 = torch.randn(5, 6, 7, 8) # (..., D1, D2)\n",
    "\n",
    "T2 = torch.randn(5, 1, 8, 9) # (..., D2, D3)\n",
    "\n",
    "# batch matrix multiplication (torch.matmul or @)\n",
    "T_matmul = torch.matmul(T1, T2) # (..., D1, D3)\n",
    "\n",
    "# The \n",
    "print('torch.bmm(T1, T2) has shape', T_matmul.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensor Multiplication\n",
    "\"\"\"\n",
    "\n",
    "# For mor complex multiplications, we can use torch.einsum\n",
    "\n",
    "T1 = torch.randn(5, 6, 7)\n",
    "T2 = torch.randn(6, 7, 8, 9)\n",
    "# We want to dot product the dimensions with size 6 and 7 (treating them as a long vector and do the dot product)\n",
    "\n",
    "T_dot = torch.einsum('ijk, jklm -> ilm', T1, T2)\n",
    "\n",
    "print('torch.einsum(\"ijk, jklm -> ilm\", T1, T2) has shape', T_dot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction operations\n",
    "\n",
    "You can take the sum, mean, standard deviation (std), product, ... of a matrix along certain dimensions. These are called reduction, because they reduce ndim by default. You can choose to keep the reduced dimension by passing `keepdim=True`. All these are both torch functions (`torch.mean(T, dim=..., keepdim=...)`) and also methods of a tensor object (`T.mean(dim=..., keepdim=...)`). Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "T = torch.randn(5, 6, 7)\n",
    "print('T has shape', T.shape)\n",
    "\n",
    "T_sum2 = T.sum(dim=2)\n",
    "print('T.sum(dim=2) has shape', T_sum2.shape)\n",
    "\n",
    "# Summ across multiple dimensions:\n",
    "T_sum01 = T.sum(dim=(0, 1))\n",
    "print('T.sum(dim=(0, 1)) has shape', T_sum01.shape)\n",
    "\n",
    "# Keeping the dimensions that your perform your operations along. The size of those dimensions will be 1\n",
    "T_sum_keepdim = T.sum(dim=2, keepdim=True)\n",
    "print('T.sum(dim=2, keepdim=True) has shape', T_sum_keepdim.shape)\n",
    "\n",
    "# Same rules for:\n",
    "\n",
    "# mean()\n",
    "# std()\n",
    "# var()\n",
    "# max()\n",
    "# min()\n",
    "# argmax()\n",
    "# argmin()\n",
    "# prod()\n",
    "\n",
    "# Try them out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
