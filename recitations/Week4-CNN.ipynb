{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a CNN classifier\n",
    "\n",
    "By now you have learned about convolutional neural networks and regularization techniques. In this notebook, we will put everything together to train a convolutional network on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from typing import Sequence, Union\n",
    "\n",
    "# For interactive plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except ImportError:\n",
    "    pass\n",
    "try:\n",
    "    %matplotlib widget\n",
    "except:\n",
    "    os.system('pip install ipympl -qq')\n",
    "    %matplotlib widget\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    Device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    Device = 'mps'\n",
    "print(f'Device: {Device}')\n",
    "\n",
    "\n",
    "def save_yaml(config: dict, path: str):\n",
    "    with open(path, 'w') as f:\n",
    "        yaml.dump(config, f, sort_keys=False, default_flow_style=None)\n",
    "\n",
    "\n",
    "def load_yaml(path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "def print_tensor_info(\n",
    "        name: str, \n",
    "        tensor, # torch.Tensor\n",
    "        ):\n",
    "    print(f'{name}')\n",
    "    print(20*'-')\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        print(f'It is {type(tensor).__name__}!')\n",
    "        print(20*'='+'\\n')\n",
    "        return\n",
    "    # print name, shhape, dtype, device, require_grad\n",
    "    print(f'shape: {tensor.shape}')\n",
    "    print(f'dtype: {tensor.dtype}')\n",
    "    print(f'device: {tensor.device}')\n",
    "    print(f'requires_grad: {tensor.requires_grad}')\n",
    "    print(20*'='+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'CIFAR10'\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    root = data_path,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.ToDtype(torch.float32, scale=True), # to [0, 1]\n",
    "        v2.Lambda(lambda x: x-0.5), # to [-0.5, 0.5]\n",
    "        ])\n",
    "        )\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root = data_path,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True), # to [0, 1]\n",
    "        v2.Lambda(lambda x: x-0.5), # to [-0.5, 0.5]\n",
    "        ])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_data[39]\n",
    "print_tensor_info('x', x)\n",
    "print_tensor_info('y', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# helper class to visualize image data interactively in Jupyter Notebook\n",
    "class ImageDataViz:\n",
    "    \"\"\"\n",
    "    An interactive image data visualzation tool inside Juptyer Notebook.\n",
    "    Make sure to use the magic command: %matplotlib widget\n",
    "    \"\"\"\n",
    "    def __init__(self, data: Dataset):\n",
    "        self.data = data\n",
    "        self.n_samples = len(data)\n",
    "        self.index = widgets.IntSlider(\n",
    "            value=0, \n",
    "            min=0, \n",
    "            max=self.n_samples-1, \n",
    "            step=1, \n",
    "            description='Index', \n",
    "            continuous_update=True,\n",
    "            layout=widgets.Layout(width='40%'),\n",
    "            )\n",
    "\n",
    "    def update(self, index: int):\n",
    "        x, y = self.data[index]\n",
    "        image = x.moveaxis(0, -1).squeeze().numpy()\n",
    "        self.img.set_data(image)\n",
    "        self.ax.set_title(f'Label: {cifar10_classes[y]}')\n",
    "\n",
    "    def show(self):\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        x, y = self.data[0]\n",
    "        image = x.moveaxis(0, -1).squeeze().numpy() # indexing the channel dimension with 0\n",
    "        self.img = self.ax.imshow(image)\n",
    "        self.ax.axis('off')\n",
    "        self.ax.set_title(f'Label: {cifar10_classes[y]}')\n",
    "        widgets.interact(self.update, index=self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = ImageDataViz(train_data)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        # Input and output information:\n",
    "        input_shape: Sequence[int], # assumed to be (H, W) where H and W are powers of 2\n",
    "\n",
    "        # Convolutional layers:\n",
    "        conv_channels: Sequence[int],\n",
    "\n",
    "        # Fully connected layers:\n",
    "        fc_dims: Sequence[int] = [],\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        # to keep track of the shape of data through the network:\n",
    "        shape = list(input_shape)\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        # Convolutional layers:\n",
    "        self.n_conv_layers = len(conv_channels)\n",
    "        conv_channels = [3] + list(conv_channels)\n",
    "        for i in range(self.n_conv_layers):\n",
    "            self.layers[f'conv-{i:02d}'] = nn.Conv2d(\n",
    "                in_channels = conv_channels[i],\n",
    "                out_channels = conv_channels[i+1],\n",
    "                kernel_size = 3,\n",
    "                stride = 1,\n",
    "                padding = 'same',\n",
    "                )\n",
    "            self.layers[f'conv-bn-{i:02d}'] = nn.BatchNorm2d(conv_channels[i+1])\n",
    "            \n",
    "            # keep track of the shape (we are going to do maxpooling with stride 2)\n",
    "            shape = [s//2 for s in shape]\n",
    "            assert all(s > 0 for s in shape), f'Invalid shape reached at layer {i}: {shape}'\n",
    "\n",
    "        self.layers['conv-dropout'] = nn.Dropout2d(p=0.1)\n",
    "\n",
    "        # Fully connected layers:\n",
    "        self.n_fc_layers = len(fc_dims)\n",
    "        fc_dims = [conv_channels[-1]*shape[0]*shape[1]] + list(fc_dims)\n",
    "        for i in range(self.n_fc_layers):\n",
    "            self.layers[f'fc-{i:02d}'] = nn.Linear(\n",
    "                in_features = fc_dims[i],\n",
    "                out_features = fc_dims[i+1],\n",
    "                )\n",
    "            self.layers[f'fc-bn-{i:02d}'] = nn.BatchNorm1d(fc_dims[i+1])\n",
    "\n",
    "        self.layers['fc-dropout'] = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.layers['out'] = nn.Linear(fc_dims[-1], 10)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.FloatTensor, # input data of shape (batch_size, in_channels, H, W)\n",
    "            ) -> torch.FloatTensor: # outputs logits of shape (batch_size, num_classes)\n",
    "\n",
    "        for i in range(self.n_conv_layers):\n",
    "            x = self.layers[f'conv-{i:02d}'](x)\n",
    "            x = self.layers[f'conv-bn-{i:02d}'](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.layers['conv-dropout'](x)\n",
    "            x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1) # (B, C, H, W) -> (B, C*H*W)\n",
    "\n",
    "        for i in range(self.n_fc_layers):\n",
    "            x = self.layers[f'fc-{i:02d}'](x)\n",
    "            x = self.layers[f'fc-bn-{i:02d}'](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.layers['fc-dropout'](x)\n",
    "\n",
    "        x = self.layers['out'](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str = Device,\n",
    "    pbar: bool = False,\n",
    "    ):\n",
    "    if pbar:\n",
    "        train_pbar = tqdm(\n",
    "            train_loader,\n",
    "            desc = 'training',\n",
    "            unit = 'batch',\n",
    "            leave = False,\n",
    "        )\n",
    "    else:\n",
    "        train_pbar = train_loader\n",
    "\n",
    "    model.train().to(device)\n",
    "\n",
    "    for x, y in train_pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if pbar:\n",
    "            train_pbar.set_postfix_str(f'loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader, # can be train_loader or val_loader or test_loader\n",
    "    loss_fn: nn.Module,\n",
    "    device: str = Device,\n",
    "    pbar: bool = False,\n",
    "    ):\n",
    "    assert loss_fn.reduction in ['mean', 'sum'], 'Invalid reduction method!'\n",
    "    if pbar:\n",
    "        val_pbar = tqdm(\n",
    "            data_loader,\n",
    "            desc = 'evaluating',\n",
    "            unit = 'batch',\n",
    "            leave = False,\n",
    "            )\n",
    "    else:\n",
    "        val_pbar = data_loader\n",
    "\n",
    "    model.eval().to(device)\n",
    "    \n",
    "    n = 0\n",
    "    Loss = 0.\n",
    "    Accuracy = 0.\n",
    "\n",
    "    for x, y in val_pbar:\n",
    "        b = len(x)\n",
    "        n += b\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        if loss_fn.reduction == 'mean':\n",
    "            Loss += loss.item()*b\n",
    "        elif loss_fn.reduction == 'sum':\n",
    "            Loss += loss.item()\n",
    "\n",
    "        Accuracy += (y_pred.argmax(dim=-1) == y).sum().item()\n",
    "        if pbar:\n",
    "            val_pbar.set_postfix_str(f'loss: {Loss/n:.4f}, accuracy: {Accuracy/n:.4f}')\n",
    "\n",
    "    return Loss/n, Accuracy/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    \"\"\"\n",
    "    Logs training and validation loss and plots them in real-time in a Jupyter notebook.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            n_epochs: int,\n",
    "            plot_freq: int = 0, # plot every plot_freq epochs. 0 for no plotting\n",
    "            ):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_accs = []\n",
    "        self.test_accs = []\n",
    "        self.epoch = 0\n",
    "        self.n_epochs = n_epochs\n",
    "        self.plot_freq = plot_freq\n",
    "        if self.plot_freq > 0:\n",
    "            self.plot_results()\n",
    "        \n",
    "        self.keys = ['train_losses', 'test_losses', 'train_accs', 'test_accs', 'epoch', 'n_epochs']\n",
    "\n",
    "    def plot_results(self):\n",
    "        self.fig, (self.loss_ax, self.acc_ax) = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "        xtickstep = max(1, self.n_epochs//10)\n",
    "        xticks = list(range(0, self.n_epochs+1, xtickstep))\n",
    "        if xticks[-1] != self.n_epochs:\n",
    "            xticks.append(self.n_epochs)\n",
    "\n",
    "        # Loss plot:\n",
    "        self.train_curve, = self.loss_ax.plot(\n",
    "            range(1, self.epoch+1), \n",
    "            self.train_losses,\n",
    "            'o-', \n",
    "            label = 'train',\n",
    "            )\n",
    "        self.val_curve, = self.loss_ax.plot(\n",
    "            range(1, self.epoch+1), \n",
    "            self.test_losses, \n",
    "            'o-',\n",
    "            label = 'test'\n",
    "            )\n",
    "        self.loss_ax.set_xlim(0, self.n_epochs+1)\n",
    "        self.loss_ax.set_xlabel('Epoch')\n",
    "        self.loss_ax.set_xticks(xticks)\n",
    "        self.loss_ax.set_ylabel('Loss')\n",
    "        self.loss_ax.set_title('Loss Learning Curve')\n",
    "        self.loss_ax.legend(loc='upper right')\n",
    "        self.loss_ax.grid(linestyle='--')\n",
    "        self.loss_text = self.loss_ax.text(1.01, 1.0, '', transform=self.loss_ax.transAxes, va='top', ha='left')\n",
    "\n",
    "        # Accuracy plot:\n",
    "        self.train_acc_curve, = self.acc_ax.plot(\n",
    "            range(1, self.epoch+1),\n",
    "            self.train_accs, \n",
    "            'o-',\n",
    "            label = 'train',\n",
    "            )\n",
    "        self.val_acc_curve, = self.acc_ax.plot(\n",
    "            range(1, self.epoch+1), \n",
    "            self.test_accs, \n",
    "            'o-',\n",
    "            label = 'test',\n",
    "            )\n",
    "        self.acc_ax.set_xlim(0, self.n_epochs+1)\n",
    "        self.acc_ax.set_xlabel('Epoch')\n",
    "        self.acc_ax.set_xticks(xticks)\n",
    "        self.acc_ax.set_ylabel('Accuracy')\n",
    "        self.acc_ax.set_title('Accuracy Learning Curve')\n",
    "        self.acc_ax.legend(loc='lower right')\n",
    "        self.acc_ax.grid(linestyle='--')\n",
    "        self.acc_text = self.acc_ax.text(1.01, 1.0, '', transform=self.acc_ax.transAxes, va='top', ha='left')\n",
    "\n",
    "    def update(\n",
    "            self, \n",
    "            train_loss: float, \n",
    "            test_loss: float, \n",
    "            train_acc: float, \n",
    "            test_acc: float,\n",
    "            ):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.train_accs.append(train_acc)\n",
    "        self.test_accs.append(test_acc)\n",
    "        self.epoch += 1\n",
    "        if self.plot_freq and self.epoch % self.plot_freq == 0:\n",
    "\n",
    "            # loss plot:\n",
    "            self.train_curve.set_data(range(1, self.epoch+1), self.train_losses)\n",
    "            self.val_curve.set_data(range(1, self.epoch+1), self.test_losses)\n",
    "            self.loss_ax.relim()\n",
    "            self.loss_ax.autoscale_view()\n",
    "            self.loss_ax.set_ylim(bottom=0.0, top=None)\n",
    "            self.loss_text.set_text(f'Epoch {self.epoch}\\n' + 20*'-' + '\\n' + f'Train Loss: {train_loss:.4f}\\nTest Loss: {test_loss:.4f}')\n",
    "\n",
    "            # accuracy plot:\n",
    "            self.train_acc_curve.set_data(range(1, self.epoch+1), self.train_accs)\n",
    "            self.val_acc_curve.set_data(range(1, self.epoch+1), self.test_accs)\n",
    "            self.acc_ax.relim()\n",
    "            self.acc_ax.autoscale_view()\n",
    "            self.acc_ax.set_ylim(bottom=None, top=1.0)\n",
    "            self.acc_text.set_text(f'Epoch {self.epoch}\\n' + 20*'-' + '\\n' + f'Train Acc: {train_acc:.4f}\\nTest Acc: {test_acc:.4f}')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            self.fig.canvas.draw()\n",
    "            clear_output(wait=True)\n",
    "            display(self.fig)\n",
    "\n",
    "    def save_results(self, path: str):\n",
    "        # saving losses and accuracies with pickle\n",
    "        with open(path, 'wb') as file:\n",
    "            pickle.dump({key: getattr(self, key) for key in self.keys}, file)\n",
    "\n",
    "    def load_results(self, path: str):\n",
    "        # loading losses and accuracies with pickle\n",
    "        with open(path, 'rb') as file:\n",
    "            results = pickle.load(file)\n",
    "            for key, value in results.items():\n",
    "                setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    # Model and data\n",
    "    save_path: str,\n",
    "    model: nn.Module,\n",
    "    train_data: Dataset,\n",
    "    test_data: Dataset,\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss_fn: nn.Module, # from nn\n",
    "    optim_name: str, # from optim\n",
    "    optim_config: dict = dict(),\n",
    "    lr_scheduler_name: Union[str, None] = None, # from lr_scheduler\n",
    "    lr_scheduler_config: dict = dict(),\n",
    "\n",
    "    # training settings:\n",
    "    n_epochs: int = 10,\n",
    "    batch_size: int = 32,\n",
    "    device: str = Device,\n",
    "\n",
    "    # progress bar and plotting:\n",
    "    train_pbar: bool = False,\n",
    "    val_pbar: bool = False,\n",
    "    plot_freq: Union[int, None] = 5,\n",
    "    save_freq: Union[int, None] = None,\n",
    "    ):\n",
    "\n",
    "    if save_freq == 0:\n",
    "        save_freq = n_epochs\n",
    "        \n",
    "    os.makedirs(f'{save_path}/checkpoints', exist_ok=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer: optim.Optimizer = optim.__getattribute__(optim_name)(model.parameters(), **optim_config)\n",
    "    if lr_scheduler_name is not None:\n",
    "        scheduler: lr_scheduler._LRScheduler = lr_scheduler.__getattribute__(lr_scheduler_name)(optimizer, **lr_scheduler_config)\n",
    "\n",
    "    epoch_pbar = tqdm(\n",
    "        range(1, n_epochs+1),\n",
    "        desc = 'epochs',\n",
    "        unit = 'epoch',\n",
    "        dynamic_ncols = True,\n",
    "        leave = True,\n",
    "        )\n",
    "\n",
    "    tracker = Tracker(n_epochs, plot_freq=plot_freq)\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "\n",
    "        train_epoch(\n",
    "            model = model,\n",
    "            train_loader = train_loader,\n",
    "            loss_fn = loss_fn,\n",
    "            optimizer = optimizer,\n",
    "            device = device,\n",
    "            pbar = train_pbar,\n",
    "            )\n",
    "\n",
    "        train_loss, train_acc = eval_epoch(\n",
    "            model = model,\n",
    "            data_loader = train_loader,\n",
    "            loss_fn = loss_fn,\n",
    "            device = device,\n",
    "            pbar = val_pbar,\n",
    "            )\n",
    "\n",
    "        test_loss, test_acc = eval_epoch(\n",
    "            model = model,\n",
    "            data_loader = test_loader,\n",
    "            loss_fn = loss_fn,\n",
    "            device = device,\n",
    "            pbar = val_pbar,\n",
    "            )\n",
    "\n",
    "        if lr_scheduler_name == 'ReduceLROnPlateau':\n",
    "            scheduler.step(train_loss)\n",
    "        elif lr_scheduler_name is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        tracker.update(train_loss, test_loss, train_acc, test_acc)\n",
    "\n",
    "        if epoch % save_freq == 0:\n",
    "            torch.save(model.state_dict(), f'{save_path}/checkpoints/epoch_{epoch}.pt')\n",
    "            tracker.save_results(f'{save_path}/results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model and training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a folder to save the results\n",
    "results_dir = 'week4-results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "model_config = dict(\n",
    "    input_shape = (32, 32),\n",
    "    conv_channels = [16, 32, 64],\n",
    "    fc_dims = [128],\n",
    "    )\n",
    "\n",
    "train_config = dict(\n",
    "    optim_name = 'Adam',\n",
    "    optim_config = dict(lr=1e-3, weight_decay=1e-5),\n",
    "    lr_scheduler_name = None,\n",
    "    lr_scheduler_config = dict(),\n",
    "    n_epochs = 20,\n",
    "    batch_size = 64,\n",
    "    )\n",
    "\n",
    "# create a folder to save the configs, model checkpoints, and results:\n",
    "n_experiments = len(os.listdir(results_dir))\n",
    "name = f'{n_experiments:02d}' # automatic\n",
    "# name = 'My_best_try'\n",
    "save_path = f'{results_dir}/{name}'\n",
    "os.makedirs(save_path, exist_ok=False) # make sure to not overwrite existing results\n",
    "save_yaml(model_config, f'{save_path}/model_config.yaml')\n",
    "save_yaml(train_config, f'{save_path}/train_config.yaml')\n",
    "\n",
    "model = CNN(**model_config)\n",
    "train(\n",
    "    save_path = save_path,\n",
    "    model = model,\n",
    "    train_data = train_data,\n",
    "    test_data = test_data,\n",
    "    loss_fn = nn.CrossEntropyLoss(),\n",
    "    train_pbar = False,\n",
    "    val_pbar = False,\n",
    "    device = Device,\n",
    "    plot_freq = 1,\n",
    "    save_freq = 1,\n",
    "    **train_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '00'\n",
    "\n",
    "# load the model config:\n",
    "model_config = load_yaml(f'{results_dir}/{name}/model_config.yaml')\n",
    "\n",
    "# create a new model with the same configuration:\n",
    "model = CNN(**model_config).to(Device)\n",
    "\n",
    "# load the results with a new tracker:\n",
    "tracker = Tracker(0)\n",
    "tracker.load_results(f'{results_dir}/{name}/results.pkl')\n",
    "\n",
    "# load the model state dict:\n",
    "state_dict = torch.load(f'{results_dir}/{name}/checkpoints/epoch_{tracker.epoch}.pt', map_location=Device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# see what the results were:\n",
    "tracker.plot_results()\n",
    "clear_output(wait=True)\n",
    "display(tracker.fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
